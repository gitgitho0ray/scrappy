{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "10_times_scrappy_doo.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOKJ0krrri1kGDipuJ+g1q4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/gitgitho0ray/scrappy/blob/master/10_times_scrappy_doo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import requests # Website connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import json # For parsing json\n",
    "import re #regex\n",
    "from time import strptime\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "from datetime import timedelta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    url = f'https://10times.com/canada/conferences?month=today&datefrom={date.today}&dateto={date.today}'\n",
    "    #magic \n",
    "    class EventDate():\n",
    "\n",
    "        def __init__(self, eventdate):\n",
    "            if len(eventdate) == 4:\n",
    "                sday=int(strptime(eventdate[0],'%d').tm_mday)\n",
    "                month=int(strptime(eventdate[2],'%b').tm_mon)\n",
    "                eday=int(strptime(eventdate[1],'%d').tm_mday)\n",
    "                year=int(strptime(eventdate[-1],'%Y').tm_year)\n",
    "                self.startdate = datetime.date(datetime(year, month, sday))\n",
    "                self.enddate = datetime.date(datetime(year, month, eday))\n",
    "            elif len(eventdate) == 5:\n",
    "                sday=int(strptime(eventdate[0],'%d').tm_mday)\n",
    "                smonth=int(strptime(eventdate[1],'%b').tm_mon)\n",
    "                eday=int(strptime(eventdate[2],'%d').tm_mday)\n",
    "                emonth=int(strptime(eventdate[3],'%b').tm_mon)\n",
    "                year=int(strptime(eventdate[-1],'%Y').tm_year)\n",
    "                self.startdate = datetime.date(datetime(year, smonth, sday))\n",
    "                self.enddate = datetime.date(datetime(year, emonth, eday))\n",
    "            else:\n",
    "                sday=int(strptime(eventdate[0],'%d').tm_mday)\n",
    "                month=int(strptime(eventdate[1],'%b').tm_mon)\n",
    "                eday=int(strptime(eventdate[0],'%d').tm_mday)\n",
    "                year=int(strptime(eventdate[-1],'%Y').tm_year)\n",
    "                self.startdate = datetime.date(datetime(year, month, sday))\n",
    "                self.enddate = datetime.date(datetime(year, month, eday))\n",
    "            \n",
    "\n",
    "    class SearchDate():   \n",
    "\n",
    "        def __init__(self,today='',endofweek='',endofthemonth=''):\n",
    "            self.today=str(datetime.now().date())\n",
    "            self.addoneweek=str((datetime.now().date()+timedelta(days=7)))\n",
    "            self.endofthemonth=str(datetime(datetime.now().year,datetime.now().month,monthrange(datetime.now().year, datetime.now().month)[1]).date())\n",
    "\n",
    "    class ScrappyDoo():\n",
    "\n",
    "        def __init__(self,url):\n",
    "            self.url = url\n",
    "\n",
    "        # def get_html(url, params=None):\n",
    "        #     h = {'user-agent':'Mozilla/5.0 (Macintosh; Intel) Gecko/20100101 Firefox/74.0'}\n",
    "        #     response = requests.get(url,params=params,headers=h)\n",
    "        #     return response.text\n",
    "\n",
    "        def get_content(self):\n",
    "\n",
    "            def get_html(url, params=None):\n",
    "                h = {'user-agent':'Mozilla/5.0 (Macintosh; Intel) Gecko/20100101 Firefox/74.0'}\n",
    "                response = requests.get(self.url,params=params,headers=None)\n",
    "                return response.text\n",
    "\n",
    "            soup = BeautifulSoup(get_html(url), 'html.parser')\n",
    "            items = soup.find_all('tr', class_='box')\n",
    "            events=[i.find('a', {'target':'_blank'}).get('href') for i in items if i.find('a', {'target':'_blank'}) !=None]\n",
    "            # event_links=[]\n",
    "            # for i in items:\n",
    "            #     if i.find('a', {'target':'_blank'}) !=None:\n",
    "            #         event_links.append(i.find('a', {'target':'_blank'}).get('href'))\n",
    "            return events\n",
    "            \n",
    "\n",
    "        def parse():\n",
    "            html = get_html(url)\n",
    "            if html.status_code == 200:\n",
    "                get_content(html.text)\n",
    "            else:\n",
    "                print('error')\n",
    "            return html\n",
    "    date = SearchDate()\n",
    "    url = f'https://10times.com/canada/conferences?month=today&datefrom={date.today}&dateto={date.today}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection = ScrappyDoo(url = url)\n",
    "collection.get_content()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get_content(get_html(url))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eventlinksdf=pd.DataFrame(collection.get_content(),columns=['URL'])\n",
    "eventlinksdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_html(url, params=None):\n",
    "    h = {'user-agent':'Mozilla/5.0 (Macintosh; Intel) Gecko/20100101 Firefox/74.0'}\n",
    "    response = requests.get(url,params=params,headers=None)\n",
    "    return response.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "urls = eventlinksdf['URL'].to_list()\n",
    "venorg=[]\n",
    "for url in urls: \n",
    "    print(url)\n",
    "    soup = BeautifulSoup(get_html(url), 'html.parser')\n",
    "    \n",
    "\n",
    "    \n",
    "    #Event info \n",
    "    eventname = soup.find('h1').get_text()\n",
    "    ####NEW NOT WORKING \n",
    "    try:\n",
    "        organizer = soup.find('h3',{'id':'org-name'}).get_text().split('\\n')[0]\n",
    "    except AttributeError:\n",
    "        organizer = soup.find('h3').get_text().split('\\n')[0]\n",
    "\n",
    "    description = soup.find('p', class_=\"desc mng word-break\").get_text(strip=True)\n",
    "    location=[i.get_text() for i in soup.find_all('p') if i.find('span') != None][0]\n",
    "    # try:\n",
    "    eventdate = EventDate(soup.select('span[content]')[0].get_text().replace('-','').split())\n",
    "    # except ValueError:\n",
    "    #     continue\n",
    "\n",
    "    rawtables = [i.find('table',class_='table noBorder mng').find_all('td') for i in soup.find_all('div', class_='row11')]\n",
    "    info=[rawtables[0][i].get_text().split(\" \",1) for i in range(len(rawtables[0]))]\n",
    "\n",
    "    # try:\n",
    "    infodata=[i[1].strip('\\n').strip() for i in info[:4]]\n",
    "    # except IndexError:\n",
    "    #     continue\n",
    "\n",
    "    time = [\" \".join(i[:2]) for i in [i.split() for i in infodata[0].split('-')]] # time \n",
    "    participants =(re.findall(r\"[0-9]+\\s-\\s[0-9]+ | [0-9]+\", infodata[2]))[0].strip()\n",
    "    tags = (', '.join(infodata[3].replace('Type','').replace('&','').split()))\n",
    "    \n",
    "    #ATTENDEES BLOCK \n",
    "    attendeename=[]\n",
    "    profilelinks=[]\n",
    "    attendeeloc=[]\n",
    "    attendeedata=[]\n",
    "\n",
    "    try:\n",
    "        for i in soup.find('div', class_=\"visitor clearfix\"):\n",
    "            try:\n",
    "                attendeedata.append([i.find('h4').get_text(),' '.join(i.get_text().replace('Connect','').split()[-2:]),i.find('a').get('href')])\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    except TypeError:\n",
    "        continue\n",
    "\n",
    "    #df list \n",
    "    venorg.append([eventname,organizer,description,location,eventdate.startdate,eventdate.enddate,time[0],time[1],tags,participants,attendeedata])\n",
    "    print('Processed')\n",
    "\n",
    "\n",
    "finaldf = pd.DataFrame(venorg,columns=['EventName',\n",
    "                                       'Organizer',\n",
    "                                       'Description',\n",
    "                                       'Address',\n",
    "                                       'StartDate',\n",
    "                                       'EndDate',\n",
    "                                       'StartTime',\n",
    "                                       'EndTime',\n",
    "                                       'Tags',\n",
    "                                       'ExpectedParticipants',\n",
    "                                       'AttendeeData'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finaldf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userurl='https://10times.com/karrass-effective-negotiating-seminar-calgary'\n",
    "soup1 = BeautifulSoup(get_html(userurl), 'html.parser')\n",
    "rawtables = [i.find('table',class_='table noBorder mng').find_all('td') for i in soup.find_all('div', class_='row11')]\n",
    "info=[rawtables[0][i].get_text().split(\" \",1) for i in range(len(rawtables[0]))]\n",
    "infodata=[i[1].strip('\\n').strip() for i in info]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "infodata=[i[1].strip('\\n').strip() for i in info]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in info[:4]:\n",
    "    print(i[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userurl='https://10times.com/judy-project'\n",
    "soup1 = BeautifulSoup(get_html(userurl), 'html.parser')\n",
    "attendeename=[]\n",
    "profilelinks=[]\n",
    "attendeeloc=[]\n",
    "attendeedata=[]\n",
    "\n",
    "#ATTENDEES BLOCK \n",
    "\n",
    "for i in soup1.find('div', class_=\"visitor clearfix\"):\n",
    "    try:\n",
    "        attendeedata.append([i.find('h4').get_text(),' '.join(i.get_text().replace('Connect','').split()[-2:]),i.find('a').get('href')])\n",
    "        attendees.append(i.find('h4').get_text()) # name\n",
    "        profilelinks.append(i.find('a').get('href')) #profilelink\n",
    "        attendeeloc.append(' '.join(i.get_text().replace('Connect','').split()[-2:])) #location \n",
    "    except AttributeError:\n",
    "        continue\n",
    "attendeedata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# items = soup.find_all('div', class_='row11')\n",
    "rawtables = [i.find('table',class_='table noBorder mng').find_all('td') for i in soup.find_all('div', class_='row11')]\n",
    "info=[rawtables[0][i].get_text().split(\" \",1) for i in range(len(rawtables[0]))]\n",
    "infodata=[i[1].strip('\\n').strip() for i in info]\n",
    "time = [\" \".join(i[:2]) for i in [i.split() for i in infodata[0].split('-')]] # time \n",
    "participants =(re.findall(r\"[0-9]+\\s-\\s[0-9]+\", infodata[2]))\n",
    "tags = (', '.join(infodata[3].replace('Type','').replace('&','').split()))\n",
    "print(tags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}